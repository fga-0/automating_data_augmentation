{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fga-0/automating_data_augmentation/blob/main/wrn_concat_raw_aug.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "path_to_data = \"drive/MyDrive/SDD/data_augmentation/cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_augment = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=path_to_data, train=True, download=True, transform=transform)\n",
    "trainset_aug = torchvision.datasets.CIFAR10(root=path_to_data, train=True, download=True, transform=transform_augment)\n",
    "testset = torchvision.datasets.CIFAR10(root=path_to_data, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_aug, test_subset_aug = utils.get_subset(trainset=trainset_aug, testset=testset, percentage=0.01)\n",
    "\n",
    "train_subset, test_subset = utils.get_subset(trainset=trainset, testset=testset, percentage=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_raw_and_AA_CIFAR = ConcatDataset([train_subset, train_subset_aug])\n",
    "train_loader_AA_CIFAR = DataLoader(train_subset_raw_and_AA_CIFAR, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# test_loader_AA_CIFAR = DataLoader(test_subset, batch_size=32, shuffle=False, num_workers=2) # inutile d'utiliser ce test loader ?\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import wide_resnet50_2\n",
    "\n",
    "# Load Wide ResNet model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# wrn = wide_resnet50_2(num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Load a pre-trained Wide ResNet-50-2 model:\n",
    "wrn = wide_resnet50_2(pretrained=True)\n",
    "# Freeze all model parameters except for the final layer:\n",
    "for param in wrn.parameters():\n",
    "    param.requires_grad = False\n",
    "# Get the number of input features for the original last layer:\n",
    "num_feature = wrn.fc.in_features\n",
    "# Replace the final classification layer to match your dataset:\n",
    "wrn.fc = nn.Linear(num_feature, 10)\n",
    "# View the structure of the new final layer (optional):\n",
    "print(wrn.fc)\n",
    "# Move the model to the GPU for accelerated training:\n",
    "wrn = wrn.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(wrn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# optimizer = optim.SGD(wrn.fc.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = optim.Adam(wrn.fc.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrn_trained_on_raw_AND_augmented_data = utils.train_WideResNet(model=wrn, trainloader=train_loader_AA_CIFAR, num_epochs=5, batch_size=32, optimizer=optimizer, criterion=criterion, device=device, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.evaluate(wrn_trained_on_raw_AND_augmented_data, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_augmentation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
