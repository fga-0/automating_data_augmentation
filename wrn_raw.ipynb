{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fga-0/automating_data_augmentation/blob/main/wrn_raw.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "path_to_data = \"drive/MyDrive/SDD/data_augmentation/cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (normalization, conversion to tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load full CIFAR-10 dataset\n",
    "# # Les images CIFAR10 sont de dimension 32x32.\n",
    "trainset = torchvision.datasets.CIFAR10(root=path_to_data, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root=path_to_data, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_trainloader_valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_trainloader_valloader(dataset=trainset,\n",
    "                                                     split=0.8,\n",
    "                                                     batch_size=128\n",
    "                                                     )\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import wide_resnet50_2\n",
    "\n",
    "# Load Wide ResNet model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pre-trained Wide ResNet-50-2 model:\n",
    "wrn = wide_resnet50_2(pretrained=True)\n",
    "# Freeze all model parameters except for the final layer:\n",
    "for param in wrn.parameters():\n",
    "    param.requires_grad = False\n",
    "# Get the number of input features for the original last layer:\n",
    "num_feature = wrn.fc.in_features\n",
    "# Replace the final classification layer to match your dataset:\n",
    "wrn.fc = nn.Linear(num_feature, 10)\n",
    "# View the structure of the new final layer (optional):\n",
    "print(wrn.fc)\n",
    "# Move the model to the GPU for accelerated training:\n",
    "wrn = wrn.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(wrn.fc.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 3\n",
    "min_delta_loss = 0.001\n",
    "min_delta_accuracy = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrn_trained = utils.train_WideResNet(model=wrn, trainloader=train_loader, valloader=val_loader, num_epochs=50, \n",
    "                                     batch_size=128, optimizer=optimizer, criterion=criterion, \n",
    "                                     device=device, scheduler=scheduler, patience=patience, \n",
    "                                     min_delta_loss=min_delta_loss, min_delta_accuracy=min_delta_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.evaluate(wrn_trained, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_augmentation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
