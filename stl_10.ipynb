{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données STL10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " STL10 contient 5000 images d'entraînement et 8000 images de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_data = \"./STL10_9696\"\n",
    "path_to_data = \"./STL10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset STL10 chargé: 5000 images d'entraînement, 8000 images de test\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformations basiques sans augmentation\n",
    "transform_base = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Redimensionnement pour le modèle\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalisation\n",
    "])\n",
    "\n",
    "# Chargement du dataset STL10\n",
    "# Les images STL10 sont de dimension 96x96.\n",
    "trainset = datasets.STL10(root=path_to_data, split='train', download=False, transform=transform_base)\n",
    "testset = datasets.STL10(root=path_to_data, split='test', download=False, transform=transform_base)\n",
    "\n",
    "# train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "# test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Dataset STL10 chargé: {len(trainset)} images d'entraînement, {len(testset)} images de test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.75686276..0.92156863].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6cf1789100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(trainset[0][0])\n",
    "plt.imshow(np.transpose(trainset[4][0],(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de l'agent LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce LSTM prend en entrée un vecteur aléatoire et génère une politique d'augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce LSTM génère une politique d'augmentation sous forme de 5 nombres entre 0 et 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class AutoAugmentLSTM(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=128, num_layers=2, output_size=5):\n",
    "        super(AutoAugmentLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # 5 valeurs pour 5 types d'augmentation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Dernière sortie de la séquence\n",
    "        return torch.sigmoid(out)  # Convertir en probabilités entre 0 et 1\n",
    "\n",
    "lstm = AutoAugmentLSTM().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération des Politiques d’Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On convertit la sortie du LSTM en transformations dynamiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seules certaines transformations sont appliquées selon la politique générée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def apply_augmentations(image, \n",
    "                        policy, \n",
    "                        replace : bool # this argument is used to specify if the data should be replaced by the augmented data or enriched.\n",
    "                        ):\n",
    "    augmentations = [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "        transforms.GaussianBlur(kernel_size=3)\n",
    "    ]\n",
    "\n",
    "    for i, aug in enumerate(augmentations):\n",
    "        if policy[i] > 0.5:  # Seuil pour activer une transformation\n",
    "            image_aug = aug(image)\n",
    "    if replace : \n",
    "        return image_aug\n",
    "    else : \n",
    "        return image, image_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle Proxy pour Tester les Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraîne un CNN léger (ResNet18) sur les données augmentées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle proxy nous permet de tester si une politique d’augmentation améliore l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un proxy simplissime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performances : \n",
    "\n",
    "    RUN_0 : \n",
    "        images 32x32\n",
    "        batch_size=64\n",
    "        20%|██        | 1/5 [00:51<03:27, 51.93s/it] Epoch [1/5], Loss: 1.7010\n",
    "        num_worker=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# proxy_model=SimpleCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un proxy moins simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "proxy_model = models.resnet18(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Modèle proxy pour évaluer la politique d'augmentation\n",
    "proxy_model = models.resnet18(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(proxy_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_proxy_model(train_loader, model, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boucle de Recherche pour Optimiser l’AutoAugment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraîne le LSTM en mettant à jour ses poids en fonction de l'amélioration des performances du modèle proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfromances : \n",
    "\n",
    "    RUN_0 : \n",
    "        images 96x96\n",
    "        batch_size=128\n",
    "        60%|██████    | 3/5 [02:47<01:52, 56.39s/it]Epoch [3/5], Loss: 1.1865\n",
    "        num_worker=4\n",
    "\n",
    "    RUN_1 : \n",
    "        images 32x32\n",
    "        batch_size=128\n",
    "        num_worker=4\n",
    "        40%|████      | 2/5 [01:55<02:53, 57.88s/it]\n",
    "\n",
    "    RUN_2 : \n",
    "        images 32x32\n",
    "        batch_size=64\n",
    "        num_worker=4\n",
    "        40%|████      | 2/5 [02:05<03:07, 62.54s/it]\n",
    "\n",
    "    RUN_3 : \n",
    "        images 32x32\n",
    "        batch_size=64\n",
    "        num_worker=8\n",
    "        20%|██        | 1/5 [01:00<04:00, 60.17s/it] Epoch [1/5], Loss: 1.1175\n",
    "\n",
    "    RUN_4 : \n",
    "        images 32x32\n",
    "        batch_size=64\n",
    "        num_worker=8\n",
    "        20%|██        | 1/5 [01:01<04:06, 61.54s/it] Epoch [1/5], Loss: 0.9135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:51<03:27, 51.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.7010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:40<06:43, 100.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m train_loader_aug \u001b[38;5;241m=\u001b[39m DataLoader(trainset_augmented, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle proxy avec la politique d'augmentation actuelle\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain_proxy_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Évaluer la précision après augmentation\u001b[39;00m\n\u001b[1;32m     22\u001b[0m accuracy_after \u001b[38;5;241m=\u001b[39m evaluate_proxy_model(proxy_model, test_loader)\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mtrain_proxy_model\u001b[0;34m(train_loader, model, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torchvision/models/resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_augmentation_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer_lstm = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "loss_lstm = nn.MSELoss()  # Comparaison avec l'amélioration attendue\n",
    "\n",
    "baseline_accuracy = 0.0\n",
    "\n",
    "for iteration in range(100):  # Nombre d'itérations de recherche\n",
    "    lstm_input = torch.randn(1, 10, 10).to(device)  # Entrée aléatoire\n",
    "    policy = lstm(lstm_input).detach().cpu().numpy()[0]  # Convertir en numpy\n",
    "\n",
    "    # Appliquer la politique générée\n",
    "    trainset_augmented = datasets.STL10(root=path_to_data, split='train', download=False,\n",
    "                                        # transform=transforms.Compose([transforms.ToTensor(), lambda img: apply_augmentations(img, policy, replace=True)])\n",
    "                                        transform = transforms.ToTensor()\n",
    "                                        )\n",
    "\n",
    "    train_loader_aug = DataLoader(trainset_augmented, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "    # Entraîner le modèle proxy avec la politique d'augmentation actuelle\n",
    "    train_proxy_model(train_loader_aug, proxy_model, criterion, optimizer)\n",
    "\n",
    "    # Évaluer la précision après augmentation\n",
    "    accuracy_after = evaluate_proxy_model(proxy_model, test_loader)\n",
    "\n",
    "    # Calculer la récompense et mettre à jour le LSTM\n",
    "    reward = accuracy_after - baseline_accuracy\n",
    "    loss = loss_lstm(torch.tensor([reward], dtype=torch.float32).to(device), lstm(lstm_input))\n",
    "    optimizer_lstm.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_lstm.step()\n",
    "\n",
    "    print(f\"Iteration {iteration}, Reward: {reward:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application de la Meilleure Politique Trouvée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois la meilleure politique identifiée, on applique la meilleure politique trouvée et on entraîne un modèle final performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = find_best_policy(lstm)  # Fonction qui extrait la meilleure politique\n",
    "print(f\"Meilleure politique trouvée : {best_policy}\")\n",
    "\n",
    "# Appliquer aux données finales\n",
    "transform_final = lambda img: apply_augmentations(img, best_policy)\n",
    "trainset_final = datasets.STL10(root=\"./data\", split='train', download=True, transform=transform_final)\n",
    "train_loader_final = DataLoader(trainset_final, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "# Entraîner le modèle final (ex: WideResNet)\n",
    "final_model = WideResNet(depth=28, width=10, num_classes=10).cuda()\n",
    "optimizer_final = optim.SGD(final_model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "train_final_model(train_loader_final, final_model, criterion, optimizer_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simple_cnn import SimpleCNN\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Initialisation du modèle\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SimpleCNN().to(device)\n",
    "\n",
    "# # Fonction de perte et optimiseur\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # Entraînement du modèle sur les données brutes\n",
    "# for epoch in range(10):  # Petit entraînement pour test\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# print(\"✅ Modèle entraîné sur données brutes !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_augmentation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
